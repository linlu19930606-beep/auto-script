import requests
import base64
import re
import socket
import json
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

# --- é…ç½®åŒº ---
SOURCES = [
    "https://raw.githubusercontent.com/free-nodes/v2rayfree/main/v202602052",
    "https://raw.githubusercontent.com/ebrasha/free-v2ray-public-list/main/all_extracted_configs.txt",
    "https://raw.githubusercontent.com/barry-far/V2ray-config/main/Splitted-By-Protocol/ss.txt"
]
MAX_THREADS = 30  # çº¿ç¨‹æ•°ï¼Œ30-50 æ¯”è¾ƒé«˜æ•ˆ
TIMEOUT = 2.5     # èŠ‚ç‚¹è¿æ¥è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

# --- åŠŸèƒ½æ¨¡å— ---

def fetch_raw_data(url):
    """æŠ“å–å¹¶åˆçº§è§£ç """
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code == 200:
            text = resp.text.strip()
            if "://" not in text:
                text += '=' * (4 - len(text) % 4)
                return base64.b64decode(text).decode('utf-8', errors='ignore')
            return text
    except:
        return ""
    return ""

def parse_ip_port(node):
    """ä»ä¸åŒåè®®æå– IP å’Œç«¯å£"""
    try:
        if node.startswith('vmess://'):
            data = json.loads(base64.b64decode(node[8:] + '===').decode('utf-8'))
            return data.get('add'), data.get('port')
        parsed = urlparse(node)
        if parsed.hostname: return parsed.hostname, parsed.port
        match = re.search(r'@([^:]+):(\d+)', node)
        if match: return match.group(1), match.group(2)
    except: pass
    return None, None

def check_node(node):
    """æµ‹è¯•èŠ‚ç‚¹é€šç•…æ€§"""
    ip, port = parse_ip_port(node)
    if not ip or not port: return None
    try:
        with socket.create_connection((str(ip), int(port)), timeout=TIMEOUT):
            return node
    except:
        return None

# --- ä¸»ç¨‹åºæµ ---

def start_workflow():
    start_time = time.time()
    print("ğŸš€ [1/3] æ­£åœ¨å…¨ç½‘æœé›†åŸå§‹èŠ‚ç‚¹...")
    
    raw_pool = []
    for url in SOURCES:
        content = fetch_raw_data(url)
        found = re.findall(r'(?:ss|vmess|vless|trojan|ssr|hy2)://[^\s<>"]+', content)
        raw_pool.extend(found)
    
    unique_raw = list(set(raw_pool))
    print(f"âœ… æœé›†å®Œæˆï¼Œå…± {len(unique_raw)} ä¸ªå”¯ä¸€èŠ‚ç‚¹ã€‚")

    print(f"âš¡ [2/3] å¯åŠ¨ {MAX_THREADS} çº¿ç¨‹è¿›è¡Œå­˜æ´»æ£€æµ‹...")
    alive_nodes = []
    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        futures = [executor.submit(check_node, n) for n in unique_raw]
        for i, f in enumerate(as_completed(futures)):
            res = f.result()
            if res: alive_nodes.append(res)
            if (i+1) % 20 == 0:
                print(f"â³ å·²æ£€æµ‹: {i+1}/{len(unique_raw)}", end='\r')

    print(f"\nâœ… æ£€æµ‹å®Œæˆï¼å­˜æ´»èŠ‚ç‚¹: {len(alive_nodes)}")

    print("ğŸ“¦ [3/3] æ­£åœ¨å°è£…è®¢é˜…æ–‡ä»¶...")
    if alive_nodes:
        sub_content = "\n".join(alive_nodes)
        encoded_sub = base64.b64encode(sub_content.encode('utf-8')).decode('utf-8')
        
        with open("my_subscribe.txt", "w", encoding="utf-8") as f:
            f.write(encoded_sub)
        
        # åŒæ—¶ä¿å­˜ä¸€ä¸ªæ˜æ–‡ç‰ˆæ–¹ä¾¿æŸ¥çœ‹
        with open("alive_list.txt", "w", encoding="utf-8") as f:
            f.write(sub_content)
            
        print(f"\nâœ¨ å…¨éƒ¨ä»»åŠ¡å·²å®Œæˆï¼è€—æ—¶: {int(time.time()-start_time)}s")
        print(f"ğŸ“‚ è®¢é˜…åŒ…å·²ç”Ÿæˆ: my_subscribe.txt")
        print(f"ğŸ“‚ æ˜æ–‡åˆ—è¡¨å·²ç”Ÿæˆ: alive_list.txt")
    else:
        print("âŒ æœªå‘ç°å¯ç”¨èŠ‚ç‚¹ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æºåœ°å€ã€‚")

if __name__ == "__main__":
    start_workflow()
